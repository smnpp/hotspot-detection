{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project\n",
    "## Touristic Hotspot in Lyon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.spatial import ConvexHull\n",
    "import os\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing missing values: 420240\n",
      "After removing missing values: 420240\n",
      "Before removing exact duplicates: 420240\n",
      "After removing exact duplicates: 168097\n",
      "Cleaned data saved in 'data/datasetCleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_table(\"../data/dataset.csv\", sep=\",\", low_memory=False)\n",
    "\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# coordonnées géographiques retenu pour Lyon = [45.75, 4.85]\n",
    "# rayon de 15 km autour de ce point\n",
    "validation_rules = {\n",
    "    'lat': lambda x: pd.api.types.is_number(x) and 45.614067767464974 <= x <= 45.88380569722158,\n",
    "    'long': lambda x: pd.api.types.is_number(x) and 4.655505238288724 <= x <= 5.042868327562071,\n",
    "    'date_taken_minute': lambda x: pd.api.types.is_number(x) and 0 <= x <= 59,\n",
    "    'date_taken_hour': lambda x: pd.api.types.is_number(x) and 0 <= x <= 23,\n",
    "    'date_taken_day': lambda x: pd.api.types.is_number(x) and 1 <= x <= 31,\n",
    "    'date_taken_month': lambda x: pd.api.types.is_number(x) and 1 <= x <= 12,\n",
    "    'date_taken_year': lambda x: pd.api.types.is_number(x) and 1839 <= x <= 2024,\n",
    "    'date_upload_minute': lambda x: pd.api.types.is_number(x) and 0 <= x <= 59,\n",
    "    'date_upload_hour': lambda x: pd.api.types.is_number(x) and 0 <= x <= 23,\n",
    "    'date_upload_day': lambda x: pd.api.types.is_number(x) and 1 <= x <= 31,\n",
    "    'date_upload_month': lambda x: pd.api.types.is_number(x) and 1 <= x <= 12,\n",
    "    'date_upload_year': lambda x: pd.api.types.is_number(x) and 1839 <= x <= 2024,\n",
    "}\n",
    "\n",
    "# Fonction de nettoyage des colonnes\n",
    "def clean_column(dataframe, column_name, validation_func):\n",
    "    dataframe[column_name] = dataframe[column_name].apply(\n",
    "        lambda x: x if validation_func(x) else np.nan\n",
    "    )\n",
    "\n",
    "for column, rule in validation_rules.items():\n",
    "    if column in data.columns:\n",
    "        clean_column(data, column, rule)\n",
    "\n",
    "print(f\"Before removing missing values: {len(data)}\")\n",
    "data_cleaned_mv = data.dropna(subset=['id', 'lat', 'long'])\n",
    "print(f\"After removing missing values: {len(data_cleaned_mv)}\")\n",
    "\n",
    "print(f\"Before removing exact duplicates: {len(data_cleaned_mv)}\")\n",
    "data_cleaned_d = data_cleaned_mv.drop_duplicates(subset=['id', 'user', 'lat', 'long'], keep='first')\n",
    "print(f\"After removing exact duplicates: {len(data_cleaned_d)}\")\n",
    "\n",
    "data_cleaned_d.to_csv('../data/datasetCleaned.csv', index=False)\n",
    "print(\"Cleaned data saved in 'data/datasetCleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Filtered data: 132626 points\n",
      "Applying K-Means...\n",
      "K-Means completed with 50 clusters\n",
      "Creating Folium map...\n",
      "Map saved as ../output/clusteringKMeans.html\n",
      "Map ready\n"
     ]
    }
   ],
   "source": [
    "# Load and filter data\n",
    "print(\"Loading data...\")\n",
    "data_path = \"../data/datasetCleaned.csv\"\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Filter for Lyon area\n",
    "lyon_bounds = {'lat_min': 45.73, 'lat_max': 45.80, 'lon_min': 4.79, 'lon_max': 4.90}\n",
    "mask = (\n",
    "    (df['lat'] >= lyon_bounds['lat_min']) & (df['lat'] <= lyon_bounds['lat_max']) &\n",
    "    (df['long'] >= lyon_bounds['lon_min']) & (df['long'] <= lyon_bounds['lon_max'])\n",
    ")\n",
    "df = df[mask]\n",
    "print(f\"Filtered data: {len(df)} points\")\n",
    "\n",
    "# Apply K-Means clustering\n",
    "print(\"Applying K-Means...\")\n",
    "X = df[['lat', 'long']].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=50, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "print(f\"K-Means completed with {len(set(df['cluster']))} clusters\")\n",
    "\n",
    "# Create Folium map\n",
    "print(\"Creating Folium map...\")\n",
    "map_clusters = folium.Map(location=[45.75, 4.85], zoom_start=13)\n",
    "\n",
    "# Generate colors for clusters\n",
    "cluster_colors = [\n",
    "    f\"#{''.join(np.random.choice(list('0123456789ABCDEF'), 6))}\"\n",
    "    for _ in range(len(df['cluster'].unique()))\n",
    "]\n",
    "\n",
    "# Add points to map\n",
    "for _, row in df.iterrows():\n",
    "    cluster_id = row['cluster']\n",
    "    color = cluster_colors[cluster_id]\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['long']],\n",
    "        radius=3,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Reverse scaling for cluster centers\n",
    "cluster_centers_original = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "\n",
    "# Add cluster centers to the map\n",
    "for i, center_coords in enumerate(cluster_centers_original):\n",
    "    folium.Marker(\n",
    "        location=[center_coords[0], center_coords[1]],\n",
    "        popup=f\"Cluster Center {i}\",\n",
    "        icon=folium.Icon(color='red', icon='info-sign')\n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Save and open the map\n",
    "map_file = \"../output/clusteringKMeans.html\"\n",
    "map_clusters.save(map_file)\n",
    "print(f\"Map saved as {map_file}\")\n",
    "\n",
    "# Open the map automatically in the browser\n",
    "webbrowser.open(f'file:///{os.path.abspath(map_file)}')\n",
    "\n",
    "print(\"Map ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data size after removing duplicates: 66819 points\n",
      "Data size after geographic filtering: 62397 points\n",
      "Data size after sampling: 12479 points\n",
      "Applying Hierarchical Clustering...\n",
      "Clustering completed with 25 clusters\n",
      "Creating Folium map...\n",
      "Map saved as ../output/clustersHierarchical.html\n",
      "Map ready\n"
     ]
    }
   ],
   "source": [
    "# Load and filter data\n",
    "print(\"Loading data...\")\n",
    "data_path = \"../data/datasetCleaned.csv\"\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Remove whitespace from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Remove duplicates based on latitude/longitude\n",
    "df = df.drop_duplicates(subset=['lat', 'long'])\n",
    "print(f\"Data size after removing duplicates: {len(df)} points\")\n",
    "\n",
    "# Filter for Lyon area\n",
    "lyon_bounds = {'lat_min': 45.709, 'lat_max': 45.80, 'lon_min': 4.79, 'lon_max': 4.90}\n",
    "mask = (\n",
    "    (df['lat'] >= lyon_bounds['lat_min']) & (df['lat'] <= lyon_bounds['lat_max']) &\n",
    "    (df['long'] >= lyon_bounds['lon_min']) & (df['long'] <= lyon_bounds['lon_max'])\n",
    ")\n",
    "df = df[mask]\n",
    "print(f\"Data size after geographic filtering: {len(df)} points\")\n",
    "\n",
    "# Reduce density by random sampling (keep only 20% of points)\n",
    "df = df.sample(frac=0.2, random_state=42)\n",
    "print(f\"Data size after sampling: {len(df)} points\")\n",
    "\n",
    "# Apply Hierarchical Clustering\n",
    "print(\"Applying Hierarchical Clustering...\")\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[['lat', 'long']])\n",
    "\n",
    "n_clusters = 25\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "df['cluster'] = clustering.fit_predict(X_scaled)\n",
    "print(f\"Clustering completed with {n_clusters} clusters\")\n",
    "\n",
    "# Create Folium map\n",
    "print(\"Creating Folium map...\")\n",
    "map_clusters = folium.Map(location=[df['lat'].mean(), df['long'].mean()], zoom_start=13)\n",
    "\n",
    "# Add Lyon boundary\n",
    "bounds = [[lyon_bounds['lat_min'], lyon_bounds['lon_min']],\n",
    "          [lyon_bounds['lat_max'], lyon_bounds['lon_max']]]\n",
    "folium.Rectangle(bounds=bounds, color='red', weight=2, fill=False, popup='Study area').add_to(map_clusters)\n",
    "\n",
    "# Generate unique colors for each cluster\n",
    "cluster_colors = [\n",
    "    f\"#{''.join(np.random.choice(list('0123456789ABCDEF'), 6))}\"\n",
    "    for _ in range(len(df['cluster'].unique()))\n",
    "]\n",
    "\n",
    "# Add cluster points\n",
    "for _, row in df.iterrows():\n",
    "    cluster_id = row['cluster']\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['long']],\n",
    "        radius=5,\n",
    "        color=cluster_colors[cluster_id],\n",
    "        fill=True,\n",
    "        fill_opacity=0.5,\n",
    "        popup=f\"Cluster {cluster_id}\"\n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Save and open the map\n",
    "map_file = \"../output/clustersHierarchical.html\"\n",
    "map_clusters.save(map_file)\n",
    "print(f\"Map saved as {map_file}\")\n",
    "\n",
    "# Open the map automatically in the browser\n",
    "webbrowser.open(f'file:///{os.path.abspath(map_file)}')\n",
    "\n",
    "print(\"Map ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data size after removing duplicates: 66819 points\n",
      "Data size after sampling: 20046 points\n",
      "Applying DBSCAN...\n",
      "Clusters found: 333\n",
      "Noise points: 3377\n",
      "Creating Folium map...\n",
      "Map saved as ../output/clusteringDBSCAN.html\n",
      "Map ready! Opened in your browser.\n"
     ]
    }
   ],
   "source": [
    "# Load and filter data\n",
    "print(\"Loading data...\")\n",
    "data_path = \"../data/datasetCleaned.csv\"\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Remove duplicates based on latitude/longitude\n",
    "df = df.drop_duplicates(subset=['lat', 'long'])\n",
    "print(f\"Data size after removing duplicates: {len(df)} points\")\n",
    "\n",
    "# Sampling to improve performance (reduce density)\n",
    "df = df.sample(frac=0.2, random_state=42)  # Keep only 30% of points\n",
    "print(f\"Data size after sampling: {len(df)} points\")\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "print(\"Applying DBSCAN...\")\n",
    "X = df[['lat', 'long']].values\n",
    "\n",
    "eps = 0.0004\n",
    "min_samples = 4\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['cluster'] = dbscan.fit_predict(X)\n",
    "\n",
    "# Compute cluster statistics\n",
    "n_clusters = len(set(df['cluster'])) - (1 if -1 in df['cluster'] else 0)\n",
    "n_noise = sum(df['cluster'] == -1)\n",
    "print(f\"Clusters found: {n_clusters}\")\n",
    "print(f\"Noise points: {n_noise}\")\n",
    "\n",
    "# Create Folium map\n",
    "print(\"Creating Folium map...\")\n",
    "map_clusters = folium.Map(location=[df['lat'].mean(), df['long'].mean()], zoom_start=12)\n",
    "\n",
    "# Generate unique colors for each cluster\n",
    "cluster_colors = ['#808080']  # Gray for noise points\n",
    "cluster_colors += [\n",
    "    f\"#{''.join(np.random.choice(list('0123456789ABCDEF'), 6))}\"\n",
    "    for _ in range(n_clusters)\n",
    "]\n",
    "\n",
    "# Add cluster points\n",
    "for _, row in df.iterrows():\n",
    "    cluster_id = row['cluster']\n",
    "    color_idx = cluster_id + 1 if cluster_id >= 0 else 0  # Gray for noise\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['long']],\n",
    "        radius=3,\n",
    "        color=cluster_colors[color_idx],\n",
    "        stroke=False if cluster_id < 0 else True,\n",
    "        fill=True,\n",
    "        fill_opacity=0.5 if cluster_id >= 0 else 0.6\n",
    "    ).add_to(map_clusters)\n",
    "\n",
    "# Add convex hulls for clusters\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_points = df[df['cluster'] == cluster_id][['lat', 'long']].values\n",
    "    if len(cluster_points) >= 3:\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            folium.Polygon(\n",
    "                locations=[[point[0], point[1]] for point in hull_points],\n",
    "                color=cluster_colors[cluster_id + 1],\n",
    "                weight=2,\n",
    "                fill=True,\n",
    "                fill_color=cluster_colors[cluster_id + 1],\n",
    "                fill_opacity=0.2\n",
    "            ).add_to(map_clusters)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating polygon for cluster {cluster_id}: {e}\")\n",
    "\n",
    "# Save and open the map\n",
    "map_file = \"../output/clusteringDBSCAN.html\"\n",
    "map_clusters.save(map_file)\n",
    "print(f\"Map saved as {map_file}\")\n",
    "\n",
    "# Open the map automatically in the browser\n",
    "webbrowser.open(f'file:///{os.path.abspath(map_file)}')\n",
    "\n",
    "print(\"Map ready! Opened in your browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
